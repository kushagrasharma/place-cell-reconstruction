{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import cycle, product\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "TESTING = True\n",
    "TIME_BUCKET_LENGTH = .1\n",
    "LOCATION_BUCKET_LENGTH = .2 # length is 1/LOCATION_BUCKET_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concerns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location data has sudden jumps that appear to be discontinouities when graphed out. Additionally, there is data for x and y positions, and both x and y vary to the degree expected if there were two degrees of freedom. I chose to use only x data for this reconstruction.\n",
    "\n",
    "Does the calculation we did still hold if we have multiple spikes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSQL DISCOVERY QUERIES FOR METADATA (used to find e.g. all linear experiments)\\nselect f.topdir, f.session from session s, file f where s.behavior = 'linear' and s.session=f.session \\norder by f.topdir desc;\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SQL DISCOVERY QUERIES FOR METADATA (used to find e.g. all linear experiments)\n",
    "select f.topdir, f.session from session s, file f where s.behavior = 'linear' and s.session=f.session \n",
    "order by f.topdir desc;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "| Record data from channels (amplify by 1000x, record at 20Khz (20,000Hz) or 32,552Hz sample\n",
    " | rate, bandpass 1-5kHz). Recordings made by either DataMax recording device\n",
    " | (DataMax system; RC Electronics) at 20kHz, or a NeuraLynx recording device\n",
    " | (NeuraLynx system) at 32,552Hz. Data sets recorded using DataMax (20KHz) are:\n",
    " | ec012, ec013, ec014, ec016, f01_m, g01_m, i01_m, j01_m. Data sets recorded using\n",
    " | NeuraLynx (32,552Hz) are: gor01, pin01, vvp01, ec014 (ec014.n329 only, all other\n",
    " | sessions from rat ec014 were recorded by DataMax). The sampling frequency is also\n",
    " | available in .xml files.\n",
    "\"\"\"\n",
    "def get_freq(session_dir):\n",
    "    frequency_is_32552 = [\"gor01*\", \"pin01*\", \"vvp01*\", \"ec014\\.n329*\"]\n",
    "    frequency_is_32552 = [re.compile(x) for x in frequency_is_32552]\n",
    "    for freq_reg in frequency_is_32552:\n",
    "        if freq_reg.match(session_dir):\n",
    "            return 32552\n",
    "    return 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_experiments = {'ec016.59': ['ec016.1047'],\n",
    " 'ec016.53': ['ec016.931'],\n",
    " 'ec016.49': ['ec016.850'],\n",
    " 'ec016.45': ['ec016.749'],\n",
    " 'ec016.44': ['ec016.733'],\n",
    " 'ec016.41': ['ec016.674'],\n",
    " 'ec016.19': ['ec016.269'],\n",
    " 'ec016.17': ['ec016.233', 'ec016.234'],\n",
    " 'ec014.36': ['ec014.639'],\n",
    " 'ec014.29': ['ec014.468'],\n",
    " 'ec013.56': ['ec013.978', 'ec013.979', 'ec013.980'],\n",
    " 'ec013.55': ['ec013.965', 'ec013.966', 'ec013.969'],\n",
    " 'ec013.54': ['ec013.949', 'ec013.950', 'ec013.951'],\n",
    " 'ec013.53': ['ec013.932', 'ec013.933', 'ec013.934'],\n",
    " 'ec013.51': ['ec013.906', 'ec013.910', 'ec013.911'],\n",
    " 'ec013.49': ['ec013.874', 'ec013.880', 'ec013.881', 'ec013.882'],\n",
    " 'ec013.48': ['ec013.859', 'ec013.860', 'ec013.861'],\n",
    " 'ec013.47': ['ec013.840', 'ec013.842', 'ec013.843'],\n",
    " 'ec013.45': ['ec013.799', 'ec013.805', 'ec013.806', 'ec013.807'],\n",
    " 'ec013.44': ['ec013.788'],\n",
    " 'ec013.42': ['ec013.761', 'ec013.762', 'ec013.764'],\n",
    " 'ec013.41': ['ec013.737', 'ec013.738', 'ec013.739'],\n",
    " 'ec013.40': ['ec013.718', 'ec013.719', 'ec013.720'],\n",
    " 'ec013.39': ['ec013.683', 'ec013.684', 'ec013.685'],\n",
    " 'ec013.38': ['ec013.669', 'ec013.670', 'ec013.671'],\n",
    " 'ec013.37': ['ec013.639', 'ec013.642', 'ec013.643'],\n",
    " 'ec013.36': ['ec013.626', 'ec013.627', 'ec013.628'],\n",
    " 'ec013.35': ['ec013.589', 'ec013.599', 'ec013.600', 'ec013.601'],\n",
    " 'ec013.34': ['ec013.573', 'ec013.574', 'ec013.576'],\n",
    " 'ec013.33': ['ec013.554', 'ec013.555', 'ec013.556'],\n",
    " 'ec013.32': ['ec013.531', 'ec013.532', 'ec013.533'],\n",
    " 'ec013.31': ['ec013.502', 'ec013.503', 'ec013.504'],\n",
    " 'ec013.30': ['ec013.454', 'ec013.465', 'ec013.466', 'ec013.469'],\n",
    " 'ec013.29': ['ec013.440', 'ec013.441', 'ec013.442'],\n",
    " 'ec013.28': ['ec013.395', 'ec013.412', 'ec013.413', 'ec013.414'],\n",
    " 'ec013.27': ['ec013.374', 'ec013.375', 'ec013.386', 'ec013.387', 'ec013.388'],\n",
    " 'ec013.21': ['ec013.251', 'ec013.252'],\n",
    " 'ec013.18': ['ec013.205', 'ec013.206', 'ec013.208'],\n",
    " 'ec013.15': ['ec013.156', 'ec013.157'],\n",
    " 'ec012ec.27': ['ec012ec.560', 'ec012ec.561'],\n",
    " 'ec012ec.24': ['ec012ec.503', 'ec012ec.504'],\n",
    " 'ec012ec.22': ['ec012ec.465', 'ec012ec.466', 'ec012ec.467'],\n",
    " 'ec012ec.21': ['ec012ec.444', 'ec012ec.445'],\n",
    " 'ec012ec.18': ['ec012ec.374', 'ec012ec.375'],\n",
    " 'ec012ec.17': ['ec012ec.356', 'ec012ec.357'],\n",
    " 'ec012ec.14': ['ec012ec.269', 'ec012ec.270', 'ec012ec.271'],\n",
    " 'ec012ec.13': ['ec012ec.239', 'ec012ec.240']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datadirs():\n",
    "    dirs = []\n",
    "    if TESTING:\n",
    "        dirs.append('data/ec013.40/ec013.719/')\n",
    "    else:\n",
    "        for key, value in linear_experiments.iteritems():\n",
    "            for session in value:\n",
    "                dirs.append('data/' + key + '/' + session + '/')\n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:44: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = defaultdict(lambda: {})\n",
    "# parse time, location, and spike data for each electrode\n",
    "dirs = get_datadirs()\n",
    "for session_dir in dirs:\n",
    "    location_reg = re.compile(\".*\\.whl\")\n",
    "    time_reg = re.compile(\".*\\.res\\.*\")\n",
    "    cluster_reg = re.compile(\".*\\.clu\\.*\")\n",
    "    freq = get_freq(session_dir)\n",
    "    files = [f for f in listdir(session_dir) if isfile(join(session_dir, f))]\n",
    "    data_files = defaultdict(list)\n",
    "    for file in files:\n",
    "        if location_reg.match(file):\n",
    "            location_df = pd.read_csv(join(session_dir, file), delimiter='\\t', header=None)\n",
    "            location_df['time'] = location_df.index / 39.0625\n",
    "            location_df['time'] = pd.to_timedelta(location_df['time'], unit=\"sec\")\n",
    "#             location_df.drop_duplicates(subset=[0,1,2,3], keep=False, inplace=True)\n",
    "            data[session_dir]['location'] = location_df\n",
    "        elif time_reg.match(file):\n",
    "            electrode_num = int(file.rsplit('.', 1)[1])\n",
    "            time_series = pd.read_csv(join(session_dir, file), delimiter='\\n', header=None, squeeze=True)\n",
    "            time_series /= freq\n",
    "            time_series = pd.to_timedelta(time_series, unit=\"sec\")\n",
    "            if electrode_num not in data[session_dir]:\n",
    "                data[session_dir][electrode_num] = pd.DataFrame()\n",
    "            data[session_dir][electrode_num]['time'] = time_series\n",
    "        elif cluster_reg.match(file):\n",
    "            electrode_num = int(file.rsplit('.', 1)[1])\n",
    "            series = pd.read_csv(join(session_dir, file), delimiter='\\n', header=None, squeeze=True)\n",
    "            n_clusters = series.iloc[0]\n",
    "            series = series.iloc[1:]\n",
    "            series.reset_index(drop=True, inplace=True)\n",
    "            if electrode_num not in data[session_dir]:\n",
    "                data[session_dir][electrode_num] = pd.DataFrame()\n",
    "            data[session_dir][electrode_num]['spikes'] = series\n",
    "            \n",
    "# combine data from each electrode into one concantenated dataframe\n",
    "concantenated_data = {}\n",
    "for session, session_data in data.items():\n",
    "    concantenated_spikes = pd.DataFrame(columns=['spikes', 'time'])\n",
    "    for electrode_num, spike_data in session_data.items():\n",
    "        if electrode_num == \"location\":\n",
    "            continue\n",
    "        spike_data['spikes'] = spike_data['spikes'].apply(lambda x: str(electrode_num) + '-' + str(x))\n",
    "        concantenated_spikes = pd.concat([concantenated_spikes, spike_data], ignore_index=True)\n",
    "    concantenated_spikes\n",
    "    concantenated_data[session] = concantenated_spikes.sort_values('time').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://crcns.org/forum/using-datasets/62983963#926694452 - sampling rate of .whl is 39.06 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_spikes(spike_train, bucket_size=TIME_BUCKET_LENGTH):\n",
    "    spike_train.loc[len(spike_train)-1] = [float('nan'), pd.Timedelta(0, unit=\"sec\")]\n",
    "    spike_train = spike_train.sort_values('time').reset_index(drop=True)\n",
    "    binned = spike_train.groupby(pd.Grouper(key='time', freq='{}S'.format(bucket_size), \n",
    "                                            base=spike_train['time'][0].seconds))\n",
    "    binned = binned['spikes'].apply(lambda x: x.value_counts())\n",
    "    binned = pd.DataFrame(binned).unstack(fill_value=0)\n",
    "    binned.columns = binned.columns.droplevel()\n",
    "    binned = binned.reindex(sorted(binned.columns), axis=1)\n",
    "    binned.columns = list(range(len(binned.columns)))\n",
    "    return binned\n",
    "\n",
    "def x_round(x, round_num):\n",
    "    if x < 0:\n",
    "        return x\n",
    "    return round(x*round_num)/round_num\n",
    "\n",
    "def bucket_location(locations, bucket_size=TIME_BUCKET_LENGTH, location_bucket_length=LOCATION_BUCKET_LENGTH):\n",
    "    locations.loc[len(locations)-1] = ([float('nan')] * (len(locations.columns)-1))+ [pd.Timedelta(0, unit=\"sec\")]\n",
    "    locations = locations.sort_values('time').reset_index(drop=True)\n",
    "    locations = locations.groupby(pd.Grouper(key='time', freq='{}S'.format(bucket_size)))\n",
    "    locations = pd.DataFrame(locations.mean())\n",
    "    locations = locations.fillna(method='ffill')\n",
    "    locations = locations.apply(lambda x: x.apply(lambda y: x_round(y, location_bucket_length)))\n",
    "    return locations\n",
    "\n",
    "def poisson_mean(df, i, x, time_bucket_length):\n",
    "    return df[df.location==x][i].sum()/(len(df[df.location==x]) * time_bucket_length)\n",
    "\n",
    "def caculate_poisson(df, locations, time_bucket_length):\n",
    "    poisson = {}\n",
    "    for x_k in locations:\n",
    "        for i in list(df.columns[:-1]):\n",
    "            pm = poisson_mean(df, i, x_k, time_bucket_length)\n",
    "            if pm:\n",
    "                poisson[i, x_k] = pm\n",
    "    return poisson\n",
    "        \n",
    "\n",
    "def MLE_x(df, spikes, x, time_bucket_length):\n",
    "    # maximum likelihood estimate for x given spikes at a time\n",
    "    max_val = float('-inf')\n",
    "    max_x = -1\n",
    "    for x_k in x:\n",
    "        sum_val = 0\n",
    "        for i in list(df.columns[:-1]):\n",
    "            pm = poisson_mean(df, i, x_k, time_bucket_length)\n",
    "            if pm:\n",
    "                sum_val += (spikes[i] * math.log(pm*time_bucket_length)) - (pm*time_bucket_length)\n",
    "        if sum_val > max_val:\n",
    "            max_x = x_k\n",
    "            max_val = sum_val\n",
    "    return max_x\n",
    "\n",
    "def MLE_p(neurons, spikes, x, time_bucket_length, poisson):\n",
    "    # poisson pre-calculated\n",
    "    max_val = float('-inf')\n",
    "    max_x = -1\n",
    "    for x_k in x:\n",
    "        sum_val = 0\n",
    "        for i in neurons:\n",
    "            if (i, x_k) in poisson:\n",
    "                pm = poisson[i, x_k]\n",
    "                sum_val += (spikes[i] * math.log(pm*time_bucket_length)) - (pm*time_bucket_length)\n",
    "        if sum_val > max_val:\n",
    "            max_x = x_k\n",
    "            max_val = sum_val\n",
    "    return max_x\n",
    "\n",
    "def location_transition_prob(x_0, x_1, v, K, V, d):\n",
    "    sigma = K * ((v/V) ** d)\n",
    "    prob = (1/(sigma * ((2 * math.pi) ** 0.5)))\n",
    "    prob *= math.e ** (-0.5 * (((x_1 - x_0)/sigma) ** 2))\n",
    "    if prob == 0:\n",
    "        # floating point arithmetic limitations cause errors later on when taking logs\n",
    "        prob = 1e-28\n",
    "    return prob\n",
    "    \n",
    "def get_average_speeds(df, locations):\n",
    "    average_speed = {}\n",
    "    no_speed_data = []\n",
    "    for location in locations:\n",
    "        at_loc = list(np.where(df.location == location)[0])\n",
    "        if(len(at_loc) < 2):\n",
    "            no_speed_data.append(location)\n",
    "            continue\n",
    "        speed = 0\n",
    "        for loc in at_loc:\n",
    "            if loc == 0:\n",
    "                speed += abs(df.location.iloc[1] - df.location.iloc[0])\n",
    "            elif loc == len(df) - 1:\n",
    "                speed += abs(df.location.iloc[-1] - df.location.iloc[-2])\n",
    "            else:\n",
    "                speed += abs(df.location.iloc[loc + 1] - df.location.iloc[loc - 1])\n",
    "        speed /= len(at_loc)\n",
    "        average_speed[location] = speed\n",
    "        if speed == 0:\n",
    "            no_speed_data.append(location)\n",
    "    overall_avg_speed = np.mean(list(average_speed.values()))\n",
    "    for location in no_speed_data:\n",
    "        average_speed[location] = overall_avg_speed\n",
    "    return average_speed\n",
    "\n",
    "def MLE_continuity_constraint(neurons, spikes, average_speed, x, x_last, K, V, d, time_bucket_length, poisson):\n",
    "    # poisson pre-calculated\n",
    "    max_val = float('-inf')\n",
    "    max_x = -1\n",
    "    for x_k in x:\n",
    "        sum_val = 0\n",
    "        for i in neurons:\n",
    "            if (i, x_k) in poisson:\n",
    "                pm = poisson[i, x_k]\n",
    "                sum_val += (spikes[i] * math.log(pm*time_bucket_length)) - (pm*time_bucket_length)\n",
    "        transition_prob = location_transition_prob(x_last, x_k, average_speed[x_k], K, V, d)\n",
    "        sum_val += math.log(transition_prob)\n",
    "        if sum_val > max_val:\n",
    "            max_x = x_k\n",
    "            max_val = sum_val\n",
    "    return max_x\n",
    "    \n",
    "\n",
    "def MSE(df, time_bucket_length):\n",
    "    msk = np.random.rand(len(df)) < .6666\n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "    error = 0\n",
    "    locations = list(set(df.location))\n",
    "    poisson = caculate_poisson(train, locations, time_bucket_length)\n",
    "    for index, row in test.iterrows():\n",
    "        x_pred = MLE_p(list(df.columns[:-1]), row[:-1], locations, time_bucket_length, poisson)\n",
    "        error += (row.iloc[-1] - x_pred) ** 2\n",
    "    error /= len(test)\n",
    "    return error\n",
    "\n",
    "def MSE_continuity_constraint(df, K, V, d, time_bucket_length):\n",
    "    msk = int(len(df) * .6666)\n",
    "    train = df.iloc[:msk]\n",
    "    test = df.iloc[msk:]\n",
    "    error = 0\n",
    "    locations = list(set(df.location))\n",
    "    average_speed = get_average_speeds(train, locations)\n",
    "    poisson = caculate_poisson(train, locations, time_bucket_length)\n",
    "    for index, row in test.iterrows():\n",
    "        x_last = df.location.iloc[np.where(df.index == index)[0] - 1].values[0]\n",
    "        x_pred = MLE_continuity_constraint(list(df.columns[:-1]), row[:-1], average_speed,\n",
    "                                           locations, x_last, K, V, d, time_bucket_length, poisson)\n",
    "        error += (row.iloc[-1] - x_pred) ** 2\n",
    "    error /= len(test)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketed = {}\n",
    "# average time between spikes is 0.002s, time between location measurements is 0.02s\n",
    "times = [5, 2, 1, 0.5, 0.3, 0.1, 0.03, 0.02, 0.01] \n",
    "# 25th percentile length difference is 0.1cm\n",
    "lengths = [50, 25, 10, 5, 2, 1, 0.5, 0.25, 0.1, 0.05]\n",
    "\n",
    "for directory in dirs:\n",
    "    for time, length in product(times, lengths):\n",
    "        if (time, length, directory) not in bucketed:\n",
    "            a = bucket_spikes(concantenated_data[directory], time)\n",
    "            b = bucket_location(data[directory]['location'], time, length)\n",
    "            b[0] = b[0].apply(lambda x: np.nan if x == -1 or x == 0 else x)\n",
    "            a['location'] = b[0]\n",
    "            a = a.dropna()\n",
    "            bucketed[time, length, directory] = a\n",
    "\n",
    "# hyperparamter tuning\n",
    "# errors = []\n",
    "# Ks = list(range(10, 30, 5))\n",
    "# Vs = list(range(10, 30, 5))\n",
    "# for directory in dirs:\n",
    "#     for time, length in product(times, lengths):\n",
    "#         a = bucketed[time, length, directory]\n",
    "#         error = MSE(a, time)\n",
    "#         for K, V in product(Ks, Vs):\n",
    "#             error_continuity_constraint = MSE_continuity_constraint(a, K, V, 0.5, time)\n",
    "#             print(\"error = {}, error_cc = {} for time {}s, length {}cm, K={}, V={}\".format(error, error_continuity_constraint, time, length, K, V, d))\n",
    "#             errors.append([time, length, K, V, error, error_continuity_constraint])\n",
    "# error_df = pd.DataFrame(errors, columns=['time', 'length', 'K', 'V', 'error', 'error_cc'])\n",
    "# error_df.to_csv(\"hyperparameter_errors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_spikes_for_neuron(df, neuron):\n",
    "    plt_df = df.copy(deep=True)\n",
    "    plt_df.index = plt_df.index.total_seconds()\n",
    "    plt_df.location.plot()\n",
    "    cycol=cycle('bgrcmky')\n",
    "    spikes_at_location = plt_df.loc[plt_df[neuron][plt_df[neuron] > 0].index]\n",
    "    plt.scatter(spikes_at_location.index, spikes_at_location.location.values)\n",
    "    plt.ylabel('Location in cm')\n",
    "    plt.xlabel('Time in s')\n",
    "    plt.title('Time vs Location for Neuron {}'.format(neuron))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data/ec013.40/ec013.719/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a6bd83810061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplot_spikes_for_neuron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucketed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data/ec013.40/ec013.719/'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'data/ec013.40/ec013.719/'"
     ]
    }
   ],
   "source": [
    "neurons = [1,10,20,30,40,50,60,70,80]\n",
    "%matplotlib inline\n",
    "for neuron in neurons:\n",
    "    plot_spikes_for_neuron(bucketed['data/ec013.40/ec013.719/'], neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df = bucketed['data/ec013.40/ec013.719/']\n",
    "poisson = caculate_poisson(df, list(set(df.location)), TIME_BUCKET_LENGTH)\n",
    "df_pos = pd.DataFrame(poisson.values(), index=poisson.keys())\n",
    "df_pos = df_pos.reset_index()\n",
    "df_pos.columns = ['neuron', 'location', 'spikes']\n",
    "# plt_df = df.copy(deep=True)\n",
    "# plt_df = plt_df.groupby('location').sum()\n",
    "cycol=cycle('bgrcmky')\n",
    "for neuron in neurons:\n",
    "    vals = df_pos[df_pos['neuron'] == neuron].sort_values('location')\n",
    "    plt.bar(vals.location, vals.spikes, color=next(cycol), label=str(neuron))\n",
    "    plt.ylabel('Number of Spikes')\n",
    "    plt.xlabel('Location in cm')\n",
    "    plt.title('Location vs Spike Count for Neuron {}'.format(neuron))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = list(set(df.location))\n",
    "predictions = []\n",
    "for index, row in df.iterrows():\n",
    "    x_pred = MLE_p(df, row[:-1], locations, TIME_BUCKET_LENGTH, poisson)\n",
    "    predictions.append((index, x_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt_df = df.copy(deep=True)\n",
    "plt_df.index = plt_df.index.total_seconds()\n",
    "plt_df.location.plot()\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.set_index(0, drop=True, inplace=True)\n",
    "pred_df.index = pred_df.index.total_seconds()\n",
    "pred_df[1].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
